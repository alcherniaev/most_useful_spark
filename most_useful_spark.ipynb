{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common Spark commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### в данном ноутбуке собраны основные фичи для новичков\n",
    "#### я написал его в первые дни работы в Сбере, когда сам изучал Spark.\n",
    "#### что-то дополнял уже впоследствии\n",
    "\n",
    "* 1) create dataframe\n",
    "* 2) joins\n",
    "* 3) execution plan\n",
    "* 4) repartition vs partitionBy\n",
    "* 5) write table\n",
    "* 6) Regex\n",
    "* 7) date\n",
    "* 8) windows func\n",
    "* 9) UDF\n",
    "* 10) Broadcast filtering\n",
    "* 11) Работа с HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "\n",
    "from collections import OrderedDict, namedtuple\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:98% !important; margin-left:1% !important; margin-right:auto !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Настройки интерфейса\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:98% !important; margin-left:1% !important; margin-right:auto !important;}</style>\"))\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "pd.set_option('display.max_rows', 70)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [рабочая] Конфигурация для ЛД 3.0 (Кластер 2.2)\n",
    "spark_home = '/opt/cloudera/parcels/SPARK2/lib/spark2'\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/cloudera/parcels/PYENV.ZNO20008661/bin/python'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/opt/python/virtualenv/jupyter/lib'\n",
    " \n",
    "sys.path.insert(0, os.path.join (spark_home,'python'))\n",
    "sys.path.insert(0, os.path.join (spark_home,'python/lib/py4j-0.10.7-src.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf, HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SparkAppName = 'test_cherniaev_aa'\n",
    "\n",
    "conf = None\n",
    "conf = SparkConf().setAppName(SparkAppName.format(str(dt.now()).split()[1][:5])).\\\n",
    "    setMaster(\"yarn-client\").\\\n",
    "    set('spark.dynamicAllocation.enabled', 'false').\\\n",
    "    set('spark.executor.memory','6g').\\\n",
    "    set('spark.driver.memory','4g').\\\n",
    "    set('spark.driver.maxResultSize','5g').\\\n",
    "    set('spark.executor.instances', '10').\\\n",
    "    set('spark.executor.cores', '2').\\\n",
    "    set('spark.yarn.executor.memoryOverhead', '2g') .\\\n",
    "    set('spark.yarn.driver.memoryOverhead', '2g') .\\\n",
    "    set('spark.sql.orc.enabled','true').\\\n",
    "    set('spark.sql.orc.filterPushdown','true').\\\n",
    "    set('spark.sql.parquet.filterPushdown','true').\\\n",
    "    set('spark.sql.autoBroadcastJoinThreshold','0').\\\n",
    "    set('spark.sql.hive.convertMetastoreOrc','true').\\\n",
    "    set('spark.ui.port', '4713').\\\n",
    "    set('spark.port.maxRetries', '113').\\\n",
    "    set('spark.sql.hive.caseSensitiveInferenceMode', 'INFER_ONLY').\\\n",
    "    set('spark.kryoserializer.buffer.max','1024m').\\\n",
    "    set('spark.yarn.am.memory','10g').\\\n",
    "    set('spark.local.dir', 'sparktmp').\\\n",
    "    set('spark.core.connection.ack.wait.timeout', '800s').\\\n",
    "    set('spark.storage.blockManagerSlaveTimeoutMs', '800s').\\\n",
    "    set('spark.shuffle.io.connectionTimeout', '800s').\\\n",
    "    set('spark.rpc.askTimeout', '800s').\\\n",
    "    set('spark.network.timeout', '3600s').\\\n",
    "    set('spark.rpc.lookupTimeout', '800s').\\\n",
    "    set('spark.shuffle.consolidateFiles', 'true').\\\n",
    "    set('spark.yarn.jars','{}/jars/*.jar'.format(spark_home)).\\\n",
    "    set('spark.shuffle.filebuffer', '64k')\n",
    "#         set('spark.sql.shuffle.partitions','100').\\    \n",
    "\n",
    "#     set('spark.yarn.queue','root.g_dl_u_corp.korsakov_dot_n_dot_n').\\\n",
    "#     set('spark.yarn.queue','root.z_transaction').\\\n",
    "#     set('spark.yarn.queue','root.g_dl_u_corp.korsakov-nn_ca-sbrf-ru').\\\n",
    "\n",
    "#  set('spark.hadoop.hive.metastore.uris', 'thrift://februs7.lab.df.sbrf.ru:9083').\\ - репозиторий ЛД 20 (ссылки на схемы - без dl_)\n",
    "#  set('hive.metastore.uris', 'thrift://pkli-chd1073.labiac.df.sbrf.ru:48869').\\ - репозиторий динамичИнфры ЛД 30 (нужны приставки dl_)\n",
    "\n",
    "#     set('spark.sql.autoBroadcastJoinThreshold','314572800').\\\n",
    "#     set('spark.hadoop.hive.metastore.uris', 'thrift://februs7.lab.df.sbrf.ru:9083').\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\t\t 2021-04-23 10:41:56.297022\n",
      "Allocated\t 2021-04-23 10:42:49.521735 | dt= 0:00:53.224713\n"
     ]
    }
   ],
   "source": [
    "# sparkContext для работы с датафреймами\n",
    "# HiveContext для последующего применения чистого sql \n",
    "st = dt.now(); print('Start\\t\\t', st)\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext = HiveContext(sc); et = dt.now(); dd = et - st; stt = et\n",
    "print('Allocated\\t', et, '| dt=', dd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFIX_TMP = 'tmp_'\n",
    "PREFIX_SBX = 'sbx_'\n",
    "PREFIX_LD20 = 'dl_' # префикс для доступа к схеме на ЛД20 из кластера ЛД30\n",
    "PREFIX_COPY_TBL = 'xx_' # Префикс пренесенных таблиц"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Создание тестовых DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+\n",
      "|         city|description| id|\n",
      "+-------------+-----------+---+\n",
      "|       Moscow|        hjg| 01|\n",
      "|       Moscow|        uuu| 02|\n",
      "|       Moscow|        kjh| 03|\n",
      "|St-Petersburg|        aaa| 04|\n",
      "|       Moscow|        lij| 01|\n",
      "|       Moscow|        bbb| 02|\n",
      "|St-Petersburg|        fff| 03|\n",
      "|St-Petersburg|        ppp| 04|\n",
      "|St-Petersburg|        jjj| 05|\n",
      "+-------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data4del_lst = [\n",
    "    Row(description='hjg', id='01', city='Moscow',), \n",
    "    Row(description='uuu', id='02', city='Moscow',), \n",
    "    Row(description='kjh', id='03', city='Moscow',), \n",
    "    Row(description='aaa', id='04', city='St-Petersburg',),\n",
    "    Row(description='lij', id='01', city='Moscow',), \n",
    "    Row(description='bbb', id='02', city='Moscow',),\n",
    "    Row(description='fff', id='03', city='St-Petersburg',), \n",
    "    Row(description='ppp', id='04', city='St-Petersburg',),\n",
    "    Row(description='jjj', id='05', city='St-Petersburg',),]\n",
    "partition_df = sqlContext.createDataFrame(data4del_lst)\n",
    "partition_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# изменение названий колонок\n",
    "new_names = ['city_name', 'descr', 'id']\n",
    "DF_data = partition_df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+\n",
      "|    city_name|descr| id|\n",
      "+-------------+-----+---+\n",
      "|       Moscow|  hjg| 01|\n",
      "|       Moscow|  uuu| 02|\n",
      "|       Moscow|  kjh| 03|\n",
      "|St-Petersburg|  aaa| 04|\n",
      "|       Moscow|  lij| 01|\n",
      "|       Moscow|  bbb| 02|\n",
      "|St-Petersburg|  fff| 03|\n",
      "|St-Petersburg|  ppp| 04|\n",
      "|St-Petersburg|  jjj| 05|\n",
      "+-------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+\n",
      "|    city_name|descr| id|\n",
      "+-------------+-----+---+\n",
      "|       Moscow|  hjg| 01|\n",
      "|       Moscow|  uuu| 02|\n",
      "|       Moscow|  kjh| 03|\n",
      "|St-Petersburg|  aaa| 04|\n",
      "|       Moscow|  lij| 01|\n",
      "|       Moscow|  bbb| 02|\n",
      "|St-Petersburg|  fff| 03|\n",
      "|St-Petersburg|  ppp| 04|\n",
      "|St-Petersburg|  jjj| 05|\n",
      "+-------------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DF_data.select(*new_names).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+\n",
      "|  k|               v|\n",
      "+---+----------------+\n",
      "| 10|          text  |\n",
      "| 20|        t e x t |\n",
      "| 31|          tx  t |\n",
      "| 10|       еще один |\n",
      "| 10|    снова text  |\n",
      "| 20|  опять t e x t |\n",
      "+---+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# создание через RDD и затем в toDF\n",
    "col_names = [('10', '  text  '),\n",
    "             ('20','  t e x t '), \n",
    "             ('31',' tx  t '), \n",
    "             ('10',' еще один '), \n",
    "             ('10', '  снова text  '),\n",
    "             ('20','  опять t e x t '),\n",
    "            ]\n",
    "tt = sc.parallelize(col_names).toDF(['k','v'])\n",
    "tt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|ul_inn_ul|val_ul|\n",
      "+---------+------+\n",
      "|       Q1|     1|\n",
      "|       W1|    10|\n",
      "|       Q1|     3|\n",
      "|       Q1|     7|\n",
      "|       W1|    20|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# напрямую указание схемы \n",
    "\n",
    "ul_data_schema = StructType([\n",
    "  StructField('ul_inn_ul', StringType(), True), \n",
    "  StructField('val_ul', LongType(), True),\n",
    "])\n",
    "\n",
    " \n",
    "ul_data = [('Q1', 1), ('W1',10), ('Q1', 3), ('Q1', 7), ('W1', 20)]\n",
    "\n",
    "ul_tt = sqlContext.createDataFrame(ul_data, schema=ul_data_schema)\n",
    "\n",
    "ul_tt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+\n",
      "|         city|description| id|\n",
      "+-------------+-----------+---+\n",
      "|        Paris|       null|  1|\n",
      "|      Saratov|    область|  2|\n",
      "|       Moscow|      город|  3|\n",
      "|St-Petersburg|      город|  4|\n",
      "|       Moscow|      город|  5|\n",
      "|       Moscow|    область|  6|\n",
      "|  Cherepovets|    область|  7|\n",
      "|       Volhov|    область|  4|\n",
      "|      Vologda|    область|  5|\n",
      "+-------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# метод через Row\n",
    "\n",
    "data4del_lst = [\n",
    "    Row(description= None, id=1, city='Paris',), \n",
    "    Row(description='область', id=2, city='Saratov',), \n",
    "    Row(description='город', id=3, city='Moscow',), \n",
    "    Row(description='город', id=4, city='St-Petersburg',),\n",
    "    Row(description='город', id=5, city='Moscow',), \n",
    "    Row(description='область', id=6, city='Moscow',),\n",
    "    Row(description='область', id=7, city='Cherepovets',), \n",
    "    Row(description='область', id=4, city='Volhov',),\n",
    "    Row(description='область', id=5, city='Vologda',),]\n",
    "test_df = sqlContext.createDataFrame(data4del_lst)\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql-like join VS df join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SortMergeJoin [id#3525L], [val_ul#3515L], Inner\n",
      ":- *Sort [id#3525L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#3525L, 200)\n",
      ":     +- *Filter isnotnull(id#3525L)\n",
      ":        +- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n",
      "+- *Sort [val_ul#3515L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(val_ul#3515L, 200)\n",
      "      +- *Filter isnotnull(val_ul#3515L)\n",
      "         +- Scan ExistingRDD[ul_inn_ul#3514,val_ul#3515L]\n"
     ]
    }
   ],
   "source": [
    "# на физическом плане джоины, написанные в sql и через датафреймы одинаковы в плане скорости\n",
    "# Catalyst сведет все в физическом плане одинаково. SQL или DF выступают в роли API\n",
    "\n",
    "test_df.join(ul_tt, test_df.id == ul_tt.val_ul).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.createOrReplaceTempView('tab1')\n",
    "ul_tt.createOrReplaceTempView('tab2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*SortMergeJoin [id#3525L], [val_ul#3515L], Inner\n",
      ":- *Sort [id#3525L ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(id#3525L, 200)\n",
      ":     +- *Filter isnotnull(id#3525L)\n",
      ":        +- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n",
      "+- *Sort [val_ul#3515L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(val_ul#3515L, 200)\n",
      "      +- *Filter isnotnull(val_ul#3515L)\n",
      "         +- Scan ExistingRDD[ul_inn_ul#3514,val_ul#3515L]\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql('select * from tab1 a join tab2 b on a.id == b.val_ul').explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyspark Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------+-----------+\n",
      "| id|         city|description|         city|description|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "| 01|       Moscow|        hjg|        Paris|       null|\n",
      "| 01|       Moscow|        lij|        Paris|       null|\n",
      "| 02|       Moscow|        bbb|      Saratov|    область|\n",
      "| 02|       Moscow|        uuu|      Saratov|    область|\n",
      "| 03|       Moscow|        kjh|       Moscow|      город|\n",
      "| 03|St-Petersburg|        fff|       Moscow|      город|\n",
      "| 04|St-Petersburg|        ppp|       Volhov|    область|\n",
      "| 04|St-Petersburg|        ppp|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        aaa|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        aaa|       Volhov|    область|\n",
      "| 05|St-Petersburg|        jjj|       Moscow|      город|\n",
      "| 05|St-Petersburg|        jjj|      Vologda|    область|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner = partition_df.join(test_df, on='id') \n",
    "df_inner.sort('id').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [id#3474, city#3472, description#3473, city#3523, description#3524]\n",
      "+- *SortMergeJoin [cast(id#3474 as bigint)], [id#3525L], Inner\n",
      "   :- *Sort [cast(id#3474 as bigint) ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(cast(id#3474 as bigint), 200)\n",
      "   :     +- *Filter isnotnull(id#3474)\n",
      "   :        +- Scan ExistingRDD[city#3472,description#3473,id#3474]\n",
      "   +- *Sort [id#3525L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#3525L, 200)\n",
      "         +- *Filter isnotnull(id#3525L)\n",
      "            +- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n"
     ]
    }
   ],
   "source": [
    "df_inner.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------+-----------+\n",
      "| id|         city|description|         city|description|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "| 01|       Moscow|        hjg|        Paris|       null|\n",
      "| 01|       Moscow|        lij|        Paris|       null|\n",
      "| 02|       Moscow|        uuu|      Saratov|    область|\n",
      "| 02|       Moscow|        bbb|      Saratov|    область|\n",
      "| 03|       Moscow|        kjh|       Moscow|      город|\n",
      "| 03|St-Petersburg|        fff|       Moscow|      город|\n",
      "| 04|St-Petersburg|        aaa|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        ppp|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        aaa|       Volhov|    область|\n",
      "| 04|St-Petersburg|        ppp|       Volhov|    область|\n",
      "| 05|St-Petersburg|        jjj|      Vologda|    область|\n",
      "| 05|St-Petersburg|        jjj|       Moscow|      город|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# бродкаст убирает шаффл, размещая одну из таблиц(маленький размер) на каждом из воркеров\n",
    "\n",
    "df_inner_broadcast = partition_df.join(broadcast(test_df), on='id') \n",
    "df_inner_broadcast.sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|       12|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner_broadcast.select(F.count('id')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Project [id#3474, city#3472, description#3473, city#3523, description#3524]\n",
      "+- *BroadcastHashJoin [cast(id#3474 as bigint)], [id#3525L], Inner, BuildRight\n",
      "   :- *Filter isnotnull(id#3474)\n",
      "   :  +- Scan ExistingRDD[city#3472,description#3473,id#3474]\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[2, bigint, false]))\n",
      "      +- *Filter isnotnull(id#3525L)\n",
      "         +- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n"
     ]
    }
   ],
   "source": [
    "df_inner_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|         city|description| id|         city|description| id|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|St-Petersburg|        jjj| 05|      Vologda|    область|  5|\n",
      "|St-Petersburg|        jjj| 05|       Moscow|      город|  5|\n",
      "|       Moscow|        lij| 01|        Paris|       null|  1|\n",
      "|       Moscow|        hjg| 01|        Paris|       null|  1|\n",
      "|St-Petersburg|        fff| 03|       Moscow|      город|  3|\n",
      "|       Moscow|        kjh| 03|       Moscow|      город|  3|\n",
      "|       Moscow|        uuu| 02|      Saratov|    область|  2|\n",
      "|       Moscow|        bbb| 02|      Saratov|    область|  2|\n",
      "|St-Petersburg|        ppp| 04|St-Petersburg|      город|  4|\n",
      "|St-Petersburg|        ppp| 04|       Volhov|    область|  4|\n",
      "|St-Petersburg|        aaa| 04|St-Petersburg|      город|  4|\n",
      "|St-Petersburg|        aaa| 04|       Volhov|    область|  4|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner2 = partition_df.join(test_df).where(partition_df.id == test_df.id)\n",
    "df_inner2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|       12|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner.select(F.count('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------+-----------+\n",
      "| id|         city|description|         city|description|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "| 05|St-Petersburg|        jjj|       Moscow|      город|\n",
      "| 05|St-Petersburg|        jjj|      Vologda|    область|\n",
      "| 01|       Moscow|        hjg|        Paris|       null|\n",
      "| 01|       Moscow|        lij|        Paris|       null|\n",
      "| 03|       Moscow|        kjh|       Moscow|      город|\n",
      "| 03|St-Petersburg|        fff|       Moscow|      город|\n",
      "| 02|       Moscow|        uuu|      Saratov|    область|\n",
      "| 02|       Moscow|        bbb|      Saratov|    область|\n",
      "| 04|St-Petersburg|        ppp|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        ppp|       Volhov|    область|\n",
      "| 04|St-Petersburg|        aaa|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        aaa|       Volhov|    область|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+-----------+-------------+-----------+\n",
      "| id|         city|description|         city|description|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "| 01|       Moscow|        hjg|        Paris|       null|\n",
      "| 01|       Moscow|        lij|        Paris|       null|\n",
      "| 02|       Moscow|        uuu|      Saratov|    область|\n",
      "| 02|       Moscow|        bbb|      Saratov|    область|\n",
      "| 03|       Moscow|        kjh|       Moscow|      город|\n",
      "| 03|St-Petersburg|        fff|       Moscow|      город|\n",
      "| 04|St-Petersburg|        aaa|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|        ppp|       Volhov|    область|\n",
      "| 04|St-Petersburg|        aaa|       Volhov|    область|\n",
      "| 04|St-Petersburg|        ppp|St-Petersburg|      город|\n",
      "| 05|St-Petersburg|        jjj|       Moscow|      город|\n",
      "| 05|St-Petersburg|        jjj|      Vologda|    область|\n",
      "+---+-------------+-----------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner = partition_df.join(test_df, on='id', how='inner')# тоже уберет дубль по id\n",
    "df_inner.sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|         city|description| id|         city|description| id|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|St-Petersburg|        jjj| 05|      Vologda|    область|  5|\n",
      "|St-Petersburg|        jjj| 05|       Moscow|      город|  5|\n",
      "|       Moscow|        lij| 01|        Paris|       null|  1|\n",
      "|       Moscow|        hjg| 01|        Paris|       null|  1|\n",
      "|St-Petersburg|        fff| 03|       Moscow|      город|  3|\n",
      "|       Moscow|        kjh| 03|       Moscow|      город|  3|\n",
      "|       Moscow|        uuu| 02|      Saratov|    область|  2|\n",
      "|       Moscow|        bbb| 02|      Saratov|    область|  2|\n",
      "|St-Petersburg|        ppp| 04|       Volhov|    область|  4|\n",
      "|St-Petersburg|        ppp| 04|St-Petersburg|      город|  4|\n",
      "|St-Petersburg|        aaa| 04|       Volhov|    область|  4|\n",
      "|St-Petersburg|        aaa| 04|St-Petersburg|      город|  4|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark может вести себя некорректно, если join по полям с одинаковым названиям. (известный баг) \n",
    "# лучше менять названия столбцов по которым идет слияние\n",
    "# протестировать, уйдет ли баг join-а по полю с одинаковым названием столбца.\n",
    "\n",
    "df_inner = partition_df.join(test_df, partition_df.id==test_df.id.alias('id2'), how='inner') # здесь уже 2 столбца id из обоих таблиц\n",
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|         city|description| id|         city|description| id|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "|St-Petersburg|        jjj| 05|       Moscow|      город|  5|\n",
      "|St-Petersburg|        jjj| 05|      Vologda|    область|  5|\n",
      "|       Moscow|        lij| 01|        Paris|       null|  1|\n",
      "|       Moscow|        hjg| 01|        Paris|       null|  1|\n",
      "|       Moscow|        kjh| 03|       Moscow|      город|  3|\n",
      "|St-Petersburg|        fff| 03|       Moscow|      город|  3|\n",
      "|       Moscow|        uuu| 02|      Saratov|    область|  2|\n",
      "|       Moscow|        bbb| 02|      Saratov|    область|  2|\n",
      "|St-Petersburg|        aaa| 04|St-Petersburg|      город|  4|\n",
      "|St-Petersburg|        aaa| 04|       Volhov|    область|  4|\n",
      "|St-Petersburg|        ppp| 04|St-Petersburg|      город|  4|\n",
      "|St-Petersburg|        ppp| 04|       Volhov|    область|  4|\n",
      "+-------------+-----------+---+-------------+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inner = partition_df.join(test_df, partition_df.id==test_df.id, how='inner')\n",
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left / Right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+-----------+\n",
      "| id|       city_1|desription_1|         city|description|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "| 01|       Moscow|         lij|        Paris|       null|\n",
      "| 01|       Moscow|         hjg|        Paris|       null|\n",
      "| 02|       Moscow|         bbb|      Saratov|    область|\n",
      "| 02|       Moscow|         uuu|      Saratov|    область|\n",
      "| 03|St-Petersburg|         fff|       Moscow|      город|\n",
      "| 03|       Moscow|         kjh|       Moscow|      город|\n",
      "| 04|St-Petersburg|         ppp|       Volhov|    область|\n",
      "| 04|St-Petersburg|         aaa|       Volhov|    область|\n",
      "| 04|St-Petersburg|         ppp|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|         aaa|St-Petersburg|      город|\n",
      "| 05|St-Petersburg|         jjj|       Moscow|      город|\n",
      "| 05|St-Petersburg|         jjj|      Vologda|    область|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left = partition_df.select('id', partition_df.city.alias('city_1'), partition_df.description.alias('desription_1')).join(test_df, on='id', how='left')\n",
    "df_left.sort('id').show()\n",
    "\n",
    "# используем alias, чтобы в новом df не было колонок с одинаковыми именами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+-----------+\n",
      "| id|       city_1|desription_1|         city|description|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "|  1|       Moscow|         lij|        Paris|       null|\n",
      "|  1|       Moscow|         hjg|        Paris|       null|\n",
      "|  2|       Moscow|         uuu|      Saratov|    область|\n",
      "|  2|       Moscow|         bbb|      Saratov|    область|\n",
      "|  3|St-Petersburg|         fff|       Moscow|      город|\n",
      "|  3|       Moscow|         kjh|       Moscow|      город|\n",
      "|  4|St-Petersburg|         ppp|St-Petersburg|      город|\n",
      "|  4|St-Petersburg|         ppp|       Volhov|    область|\n",
      "|  4|St-Petersburg|         aaa|St-Petersburg|      город|\n",
      "|  4|St-Petersburg|         aaa|       Volhov|    область|\n",
      "|  5|St-Petersburg|         jjj|      Vologda|    область|\n",
      "|  5|St-Petersburg|         jjj|       Moscow|      город|\n",
      "|  6|         null|        null|       Moscow|    область|\n",
      "|  7|         null|        null|  Cherepovets|    область|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_right = partition_df.select('id', partition_df.city.alias('city_1'), partition_df.description.alias('desription_1')).join(test_df, on='id', how='right')\n",
    "df_right.sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|count(id)|\n",
      "+---------+\n",
      "|       14|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_right.select(F.count('id')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|count(DISTINCT id)|\n",
      "+------------------+\n",
      "|                 7|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_right.select(F.countDistinct('id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#избегаем портянок с alias\n",
    "aliased_df = partition_df.select('id', \\\n",
    "                                 partition_df.city.alias('city_1'), \\\n",
    "                                 partition_df.description.alias('desription_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+-------------+-----------+\n",
      "| id|       city_1|desription_1|         city|description|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "| 01|       Moscow|         hjg|        Paris|       null|\n",
      "| 01|       Moscow|         lij|        Paris|       null|\n",
      "| 02|       Moscow|         uuu|      Saratov|    область|\n",
      "| 02|       Moscow|         bbb|      Saratov|    область|\n",
      "| 03|St-Petersburg|         fff|       Moscow|      город|\n",
      "| 03|       Moscow|         kjh|       Moscow|      город|\n",
      "| 04|St-Petersburg|         ppp|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|         aaa|St-Petersburg|      город|\n",
      "| 04|St-Petersburg|         ppp|       Volhov|    область|\n",
      "| 04|St-Petersburg|         aaa|       Volhov|    область|\n",
      "| 05|St-Petersburg|         jjj|       Moscow|      город|\n",
      "| 05|St-Petersburg|         jjj|      Vologda|    область|\n",
      "|  6|         null|        null|       Moscow|    область|\n",
      "|  7|         null|        null|  Cherepovets|    область|\n",
      "+---+-------------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full_outer = aliased_df.join(test_df, on='id', how='full')\n",
    "df_full_outer.sort('id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi / Anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+------------+\n",
      "| id|       city_1|desription_1|\n",
      "+---+-------------+------------+\n",
      "| 01|       Moscow|         hjg|\n",
      "| 01|       Moscow|         lij|\n",
      "| 02|       Moscow|         uuu|\n",
      "| 02|       Moscow|         bbb|\n",
      "| 03|St-Petersburg|         fff|\n",
      "| 03|       Moscow|         kjh|\n",
      "| 04|St-Petersburg|         aaa|\n",
      "| 04|St-Petersburg|         ppp|\n",
      "| 05|St-Petersburg|         jjj|\n",
      "+---+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_semi = aliased_df.join(test_df, on='id', how='left_semi')\n",
    "df_left_semi.sort('id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+\n",
      "| id|city_1|desription_1|\n",
      "+---+------+------------+\n",
      "+---+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_left_anti = aliased_df.join(test_df, on='id', how='leftanti')\n",
    "df_left_anti.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------------+-------------+-----------+---+\n",
      "| id|city_1|desription_1|         city|description| id|\n",
      "+---+------+------------+-------------+-----------+---+\n",
      "| 01|Moscow|         hjg|        Paris|       null|  1|\n",
      "| 01|Moscow|         hjg|      Saratov|    область|  2|\n",
      "| 01|Moscow|         hjg|       Moscow|      город|  3|\n",
      "| 01|Moscow|         hjg|St-Petersburg|      город|  4|\n",
      "| 01|Moscow|         hjg|       Moscow|      город|  5|\n",
      "| 01|Moscow|         hjg|       Moscow|    область|  6|\n",
      "| 01|Moscow|         hjg|  Cherepovets|    область|  7|\n",
      "| 01|Moscow|         hjg|       Volhov|    область|  4|\n",
      "| 01|Moscow|         hjg|      Vologda|    область|  5|\n",
      "| 02|Moscow|         uuu|        Paris|       null|  1|\n",
      "| 02|Moscow|         uuu|      Saratov|    область|  2|\n",
      "| 02|Moscow|         uuu|       Moscow|      город|  3|\n",
      "| 02|Moscow|         uuu|St-Petersburg|      город|  4|\n",
      "| 02|Moscow|         uuu|       Moscow|      город|  5|\n",
      "| 02|Moscow|         uuu|       Moscow|    область|  6|\n",
      "| 02|Moscow|         uuu|  Cherepovets|    область|  7|\n",
      "| 02|Moscow|         uuu|       Volhov|    область|  4|\n",
      "| 02|Moscow|         uuu|      Vologda|    область|  5|\n",
      "| 03|Moscow|         kjh|        Paris|       null|  1|\n",
      "| 03|Moscow|         kjh|      Saratov|    область|  2|\n",
      "+---+------+------------+-------------+-----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cross = aliased_df.crossJoin(test_df)\n",
    "df_cross.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### План запроса explain() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "SortMergeJoin [cast(id#3474 as bigint)], [id#3525L], LeftSemi\n",
      ":- *Sort [cast(id#3474 as bigint) ASC NULLS FIRST], false, 0\n",
      ":  +- Exchange hashpartitioning(cast(id#3474 as bigint), 200)\n",
      ":     +- *Project [id#3474, city#3472 AS city_1#3743, description#3473 AS desription_1#3744]\n",
      ":        +- *Filter isnotnull(id#3474)\n",
      ":           +- Scan ExistingRDD[city#3472,description#3473,id#3474]\n",
      "+- *Sort [id#3525L ASC NULLS FIRST], false, 0\n",
      "   +- Exchange hashpartitioning(id#3525L, 200)\n",
      "      +- *Project [id#3525L]\n",
      "         +- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n"
     ]
    }
   ],
   "source": [
    "df_left_semi.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CartesianProduct\n",
      ":- *Project [id#3474, city#3472 AS city_1#3743, description#3473 AS desription_1#3744]\n",
      ":  +- Scan ExistingRDD[city#3472,description#3473,id#3474]\n",
      "+- Scan ExistingRDD[city#3523,description#3524,id#3525L]\n"
     ]
    }
   ],
   "source": [
    "df_cross.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repartition \n",
    " - разделяет df на бакеты. в hdfs будет несколько файлов, в данном случае 3, сгруппированыые по id. те один и тот же id не будет в разных файлах.\n",
    " - также это сколько всего у нас будет партиций для физического исполнения на воркерах\n",
    "\n",
    "### partitionBy\n",
    " - разобьет df на директории. физически будут выглядеть как папки в hdfs. сколько значений в столбце (у нас тут city) , столько и директорий. Подходит для низкой кардинальности\n",
    " -  динамическое (можно добавлять) / статическое нельзя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "partition_df.repartition(3, 'id').write.format('csv').mode('overwrite').partitionBy('city').saveAsTable('{}.{}'.format('default', 'partition_table_csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show create table partition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tt2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-356-a192f0c6d081>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}.{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tablichka_csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tt2' is not defined"
     ]
    }
   ],
   "source": [
    "tt2.write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', 'tablichka_csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt2.repartition(2,'rn_rec').write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', 'tablichka_csv_rep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt3.write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', 'tablichka_numeric_csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt3.repartition(2,'rn_rec').write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', 'tablichka_numeric_csv_rep'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- egrul_org_id: string (nullable = true)\n",
      " |-- fld_dt: string (nullable = true)\n",
      " |-- rn_rec: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt3 = tt3.withColumn('rn_rec', tt3.rn_rec.cast(LongType()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sqlContext.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+\n",
      "|egrul_org_id|rn_rec|             fld_dt|\n",
      "+------------+------+-------------------+\n",
      "|           1|     1|2009-09-25 00:00:00|\n",
      "|           1|     2|2009-09-25 00:00:00|\n",
      "|           1|     3|2009-09-25 00:00:00|\n",
      "|           1|     4|2009-09-25 00:00:00|\n",
      "|           2|     1|2009-09-25 00:00:00|\n",
      "|           2|     2|2009-09-25 00:00:00|\n",
      "|           2|     3|2013-10-11 00:00:00|\n",
      "|           2|     4|2009-09-25 00:00:00|\n",
      "|           2|     5|2009-09-25 00:00:00|\n",
      "+------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Тестовый дата-фрейм\n",
    "cSchema = StructType([\n",
    "    StructField('egrul_org_id', LongType(), True),\n",
    "    StructField('rn_rec', LongType(), True),\n",
    "    StructField('fld_dt', StringType(), True),\n",
    "    ])\n",
    "\n",
    "# Явное добавление записей в таблицу для удаления\n",
    "data4del_lst = [\n",
    "    Row(egrul_org_id='hjg', rn_rec='01', fld_dt='2009-09-25 00:00:00',), Row(egrul_org_id='uuu', rn_rec='02', fld_dt='2009-09-25 00:00:00',),\n",
    "    Row(egrul_org_id='kjh', rn_rec='03', fld_dt='2009-09-25 00:00:00',), Row(egrul_org_id='aaa', rn_rec='04', fld_dt='2009-09-25 00:00:00',),\n",
    "    Row(egrul_org_id='lij', rn_rec='01', fld_dt='2009-09-25 00:00:00',), Row(egrul_org_id='bbb', rn_rec='02', fld_dt='2009-09-25 00:00:00',),\n",
    "    Row(egrul_org_id='fff', rn_rec='03', fld_dt='2013-10-11 00:00:00',), Row(egrul_org_id='ppp', rn_rec='04', fld_dt='2009-09-25 00:00:00',),\n",
    "    Row(egrul_org_id='jjj', rn_rec='05', fld_dt='2009-09-25 00:00:00',),]\n",
    "tt = sqlContext.createDataFrame(data4del_lst, schema=cSchema)\n",
    "tt.orderBy('egrul_org_id', 'rn_rec').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- egrul_org_id: long (nullable = true)\n",
      " |-- rn_rec: long (nullable = true)\n",
      " |-- fld_dt: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt.select(\"*\", to_timestamp(tt.fld_dt, 'yyyy-MM-dd HH:mm:ss').alias('date_col')).write.mode('overwrite').saveAsTable('{}.{}'.format('default', 'tmp_test_sav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt.select(\"*\", to_timestamp(tt.fld_dt, 'yyyy-MM-dd HH:mm:ss').alias('date_col')).write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', '\n",
    "                                                                                                                                                  '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+-------------------+----------+-----+\n",
      "|egrul_org_id|rn_rec|             fld_dt|          timestamp|      date|month|\n",
      "+------------+------+-------------------+-------------------+----------+-----+\n",
      "|           1|     1|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           1|     2|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           1|     3|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           1|     4|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           2|     1|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           2|     2|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           2|     3|2013-10-11 00:00:00|2013-10-11 00:00:00|2013-10-11|   10|\n",
      "|           2|     4|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "|           2|     5|2009-09-25 00:00:00|2009-09-25 00:00:00|2009-09-25|    9|\n",
      "+------------+------+-------------------+-------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt.select(\"*\", to_timestamp(tt.fld_dt, 'yyyy-MM-dd HH:mm:ss').alias('timestamp'), F.to_date(F.col('fld_dt'), 'yyyy-MM-dd').alias('date'), F.month('fld_dt').alias('month')).show()\n",
    "#.groupBy(\"rn_rec\").count().where(F.col('count') == 2).where('rn_rec > 2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------------------+\n",
      "|egrul_org_id|rn_rec|             fld_dt|\n",
      "+------------+------+-------------------+\n",
      "|           2|     3|2013-10-11 00:00:00|\n",
      "+------------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt.where(F.col('fld_dt') > '2010-01-01').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt.createOrReplaceTempView('my_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbx_t_team_graph_360.tmp_crt_table\n"
     ]
    }
   ],
   "source": [
    "# Сохраняем табл с бинарной разметкой ошибочных записей (чтобы не пересчитывать, возможно надо просто закешировать)\n",
    "tbl_sav = '{}crt_table'.format(PREFIX_TMP)\n",
    "print('{}.{}'.format(SCHEMA_SBX, tbl_sav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:\t 2020-07-24 14:16:59.229146\n",
      "Proc dt= 0:00:09.629188\n"
     ]
    }
   ],
   "source": [
    "sto=dt.now(); print('Start:\\t', sto )\n",
    "tt.write.format(SAVE_FORMAT).mode('overwrite').('{}.{}'.format(SCHEMA_SBX, tbl_sav))\n",
    "et=dt.now(); dd=et-sto; print('Proc dt=', dd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "| ID|        INN|DESCRIPTION|\n",
      "+---+-----------+-----------+\n",
      "|  0|12134568175|     dsfdsy|\n",
      "|  1|32134384832|     asdgfg|\n",
      "|  2|41134324812|     asdsad|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').option('delimeter', \",\").load('hdfs://nsld3/user/hive/warehouse/data_test/file1.csv', header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_new = sqlContext.read.format('csv').option('delimeter', \",\").option('header', True).load('hdfs://nsld3/user/hive/warehouse/data_test/file1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_df = df.withColumn('INN', df.INN.cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "| ID|        INN|DESCRIPTION|\n",
      "+---+-----------+-----------+\n",
      "|  0|12134568175|     dsfdsy|\n",
      "|  1|32134384832|     asdgfg|\n",
      "|  2|41134324812|     asdsad|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modified_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+\n",
      "| ID|        INN|DESCRIPTION|\n",
      "+---+-----------+-----------+\n",
      "|  0|02134568175|    1dsfdsy|\n",
      "|  1|02134384832|    1asdgfg|\n",
      "|  2|01134324812|    1asdsad|\n",
      "+---+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = sqlContext.read.format('csv').option('delimeter', ',').load('hdfs://nsld3/user/hive/warehouse/data_test/file2.csv', header=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_df2 = df2.withColumn('INN', df2.INN.cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n",
      "| ID|       INN|DESCRIPTION|\n",
      "+---+----------+-----------+\n",
      "|  0|2134568175|    1dsfdsy|\n",
      "|  1|2134384832|    1asdgfg|\n",
      "|  2|1134324812|    1asdsad|\n",
      "+---+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modified_df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified_df.write.format('csv').mode('overwrite').saveAsTable('{}.{}'.format('default', 'test_mod_csv_1'))\n",
    "modified_df2.write.format('parquet').mode('overwrite').saveAsTable('{}.{}'.format('default', 'test_mod_parquet_20'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее время отработки\t 2020-01-30 12:48:45.502338  | dt=0:16:09.172497\n"
     ]
    }
   ],
   "source": [
    "# Останов\n",
    "et = dt.now(); dd_ttl = et - stt\n",
    "print('Общее время отработки\\t {}  | dt={}'.format(et, dd_ttl))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regex (Регулярные выражения)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexp_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|regexp_extract(1123f, (\\d+), 1)|\n",
      "+-------------------------------+\n",
      "|                           1123|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select regexp_extract('1123f', '(\\\\d+)')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|word|digits|strings|\n",
      "+----+------+-------+\n",
      "|11ss|    11|     ss|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select '11ss' as word,\\\n",
    "               regexp_extract('11ss', '(\\\\d+)') as digits, \\\n",
    "               regexp_extract('11ss', '(\\\\d+)(\\\\w+)',2) as strings \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------+\n",
      "|word|digits|strings|\n",
      "+----+------+-------+\n",
      "|11ss|    11|     ss|\n",
      "+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select '11ss' as word,\\\n",
    "               regexp_extract('11ss', '(\\\\d+)') as digits, \\\n",
    "               regexp_replace('11ss', '(\\\\d+)(\\\\w+)', '$2') as strings \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexp_replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|digit|replaced|\n",
      "+-----+--------+\n",
      "|   11|      **|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select '11' as digit, regexp_replace('11', '\\\\d+', '**') as replaced\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### группы выражений вместе с regexp_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+----------+\n",
      "|word|group_int|group_all|group_text|\n",
      "+----+---------+---------+----------+\n",
      "|11ss|       11|     11ss|        ss|\n",
      "+----+---------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select '11ss' as word,\\\n",
    "               regexp_replace('11ss', '(\\\\d+)(\\\\w+)', '$1') as group_int, \\\n",
    "               regexp_replace('11ss', '(\\\\d+)(\\\\w+)', '$0') as group_all, \\\n",
    "               regexp_replace('11ss', '(\\\\d+)(\\\\w+)', '$2') as group_text\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rlike()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|validator|\n",
      "+---------+\n",
      "|    valid|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"SELECT CASE WHEN rlike('10', '^[24031]+$') THEN 'valid' ELSE 'invalid' END AS validator\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_for_df = [(1, 'sedasdas  12321 f u320321 -3  kss'), (2, '4*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))4')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_re = sc.parallelize(data_for_df).toDF(['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+\n",
      "|id |text                                         |\n",
      "+---+---------------------------------------------+\n",
      "|1  |sedasdas  12321 f u320321 -3  kss            |\n",
      "|2  |4*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))4|\n",
      "+---+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re.show(df_re.count(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_re.createOrReplaceTempView(\"df_re_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+--------+\n",
      "|id |original_text                                |num_text|\n",
      "+---+---------------------------------------------+--------+\n",
      "|1  |sedasdas  12321 f u320321 -3  kss            |12321   |\n",
      "|2  |4*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))4|4       |\n",
      "+---+---------------------------------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(r\"select id, text as original_text, regexp_extract(text, '(\\\\d+)') as num_text from df_re_table\" ).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_re_modified = sqlContext.sql(r\"select id, \\\n",
    "               text as original_text, \\\n",
    "               regexp_extract(text, '(-?\\\\d+|[0-9]+)') as num_distinct, \\\n",
    "               regexp_extract(text, '(-\\\\d+)') as neg_num, \\\n",
    "               regexp_extract(text, '(\\\\D+)') as deleted_nums, \\\n",
    "               regexp_extract(text, '(\\\\w+)') as str_distinct,\\\n",
    "               regexp_replace(text, '(\\\\D+)', '') as num_full,\\\n",
    "               regexp_replace( regexp_replace(text, '(\\\\W+)', ''), '(_+)', '') as num_and_str,\\\n",
    "               regexp_replace( regexp_replace( regexp_replace(text, '(\\\\W+)', ''), '(\\\\d+)', ''), '(_)', '') as str_full,\\\n",
    "               regexp_replace(text, '( )', '') as no_space, \\\n",
    "               regexp_replace(text, '( +)', ' ') as no_duble_space \\\n",
    "               from df_re_table\")\n",
    "\n",
    "\n",
    "#русские буквы ЪьЁй"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------------+-------+--------------------+------------+------------+--------------------+-------------+--------------------+--------------------+\n",
      "| id|       original_text|num_distinct|neg_num|        deleted_nums|str_distinct|    num_full|         num_and_str|     str_full|            no_space|      no_duble_space|\n",
      "+---+--------------------+------------+-------+--------------------+------------+------------+--------------------+-------------+--------------------+--------------------+\n",
      "|  1|sedasdas  12321 f...|       12321|     -3|          sedasdas  |    sedasdas|123213203213|sedasdas12321fu32...|sedasdasfukss|sedasdas12321fu32...|sedasdas 12321 f ...|\n",
      "|  2|4*^%^&(@*! (!DHSA...|           4|       |*^%^&(@*! (!DHSAJ...|           4|          44|        4DHSAJUdsfd4|   DHSAJUdsfd|4*^%^&(@*!(!DHSAJ...|4*^%^&(@*! (!DHSA...|\n",
      "+---+--------------------+------------+-------+--------------------+------------+------------+--------------------+-------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------------------------------------+------------+-------+-------------------------------------------+------------+------------+-------------------------+-------------+-------------------------------+----------------------------------+\n",
      "|id |original_text                                |num_distinct|neg_num|deleted_nums                               |str_distinct|num_full    |num_and_str              |str_full     |no_space                       |no_duble_space                    |\n",
      "+---+---------------------------------------------+------------+-------+-------------------------------------------+------------+------------+-------------------------+-------------+-------------------------------+----------------------------------+\n",
      "|1  |sedasdas  12321 f u320321 -3  kss            |12321       |-3     |sedasdas                                   |sedasdas    |123213203213|sedasdas12321fu3203213kss|sedasdasfukss|sedasdas12321fu320321-3kss     |sedasdas 12321 f u320321 -3 kss   |\n",
      "|2  |4*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))4|4           |       |*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))|4           |44          |4DHSAJUdsfd4             |DHSAJUdsfd   |4*^%^&(@*!(!DHSAJU)_(@#@dsfd))4|4*^%^&(@*! (!DHSAJU) _ (@#@dsfd))4|\n",
      "+---+---------------------------------------------+------------+-------+-------------------------------------------+------------+------------+-------------------------+-------------+-------------------------------+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re_modified.show(df_re_modified.count(), False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- original_text: string (nullable = true)\n",
      " |-- num_distinct: string (nullable = true)\n",
      " |-- neg_num: string (nullable = true)\n",
      " |-- deleted_nums: string (nullable = true)\n",
      " |-- str_distinct: string (nullable = true)\n",
      " |-- num_full: string (nullable = true)\n",
      " |-- num_and_str: string (nullable = true)\n",
      " |-- str_full: string (nullable = true)\n",
      " |-- no_space: string (nullable = true)\n",
      " |-- no_duble_space: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re_modified.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+-----------+--------------------+---------------------------+\n",
      "|original_text                                |len_simbols|len_simbols_no_space|len_simbols_no_double_space|\n",
      "+---------------------------------------------+-----------+--------------------+---------------------------+\n",
      "|sedasdas  12321 f u320321 -3  kss            |33         |26                  |31                         |\n",
      "|4*^%^&(@*! (!DHSAJU)     _        (@#@dsfd))4|45         |31                  |34                         |\n",
      "+---------------------------------------------+-----------+--------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_re_modified.withColumn('len_simbols', F.length(df_re_modified.original_text))\\\n",
    "              .withColumn('len_simbols_no_space', F.length(df_re_modified.no_space))\\\n",
    "              .withColumn('len_simbols_no_double_space', F.length(df_re_modified.no_duble_space))\\\n",
    "              .select('original_text', 'len_simbols', 'len_simbols_no_space','len_simbols_no_double_space').show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_re_modified.createOrReplaceTempView('re_mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[941] at RDD at PythonRDD.scala:52"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_re_modified.rdd.map(lambda line: line.split()).map(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date\n",
    "- timestamp\n",
    "- date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_data = [(1, '2020-09-15'), (2, '20-09-16 10:00:01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "| id|             date|\n",
      "+---+-----------------+\n",
      "|  1|       2020-09-15|\n",
      "|  2|20-09-16 10:00:01|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df = sc.parallelize(date_data).toDF(['id', 'date'])\n",
    "date_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+\n",
      "| id|             date|  date_mod|\n",
      "+---+-----------------+----------+\n",
      "|  1|       2020-09-15|2020-09-15|\n",
      "|  2|20-09-16 10:00:01|      null|\n",
      "+---+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.withColumn('date_mod', F.expr(\"to_date(date)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_df_mod = date_df.withColumn('date_mod', F.to_date('date'))\\\n",
    "                     .withColumn('timestamp_mod', F.to_timestamp('date', 'yy-MM-dd HH:mm:ss'))\\\n",
    "                     .withColumn('year', F.year('date_mod'))\\\n",
    "                     .withColumn('month', F.month('date_mod'))\\\n",
    "                     .withColumn('day_of_month', F.dayofmonth('date_mod'))\\\n",
    "                     .withColumn('hour', F.hour('timestamp_mod'))\\\n",
    "                     .withColumn('minute', F.minute('timestamp_mod'))\\\n",
    "                     .withColumn('second', F.second('timestamp_mod'))\\\n",
    "                     .withColumn('date_added', F.date_add('date_mod', 10))\\\n",
    "                     .withColumn('date_added_to_time_stamp', F.date_add('timestamp_mod', 10))\\\n",
    "                     .withColumn('difference', F.datediff('date_mod', 'date_added'))\n",
    "# можно ли в условии указать несколько форматов? \n",
    "# формат - не желаемый вид, как в tsql, а исходный\n",
    "# переделать через expr dateadd timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+\n",
      "| id|             date|  date_mod|      timestamp_mod|year|month|day_of_month|hour|minute|second|date_added|date_added_to_time_stamp|difference|\n",
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+\n",
      "|  1|       2020-09-15|2020-09-15|               null|2020|    9|          15|null|  null|  null|2020-09-25|                    null|       -10|\n",
      "|  2|20-09-16 10:00:01|      null|2020-09-16 10:00:01|null| null|        null|  10|     0|     1|      null|              2020-09-26|      null|\n",
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df_mod.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### более детальный DATEADD ( минуты, секунды, часы и тд) \n",
    "    F.expr('INTERVAL 30 minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+-------------------+\n",
      "| id|             date|  date_mod|      timestamp_mod|year|month|day_of_month|hour|minute|second|date_added|date_added_to_time_stamp|difference|    date_added_expr|\n",
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+-------------------+\n",
      "|  1|       2020-09-15|2020-09-15|               null|2020|    9|          15|null|  null|  null|2020-09-25|                    null|       -10|               null|\n",
      "|  2|20-09-16 10:00:01|      null|2020-09-16 10:00:01|null| null|        null|  10|     0|     1|      null|              2020-09-26|      null|2020-09-16 10:30:01|\n",
      "+---+-----------------+----------+-------------------+----+-----+------------+----+------+------+----------+------------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df_mod.withColumn('date_added_expr', date_df_mod.timestamp_mod + F.expr('INTERVAL 30 minutes')).show()\n",
    "#  col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_mod: date (nullable = true)\n",
      " |-- timestamp_mod: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day_of_month: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      " |-- second: integer (nullable = true)\n",
      " |-- date_added: date (nullable = true)\n",
      " |-- date_added_to_time_stamp: date (nullable = true)\n",
      " |-- difference: integer (nullable = true)\n",
      " |-- date_added_expr: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df_mod.withColumn('date_added_expr', date_df_mod.timestamp_mod + F.expr('INTERVAL 30 minutes')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"--надо получить\\ninn;from_dt;to_dt;cur_val\\n'001';2018-08-11;2999-12-31;100\\n'001';2015-07-04;2018-08-10;100\\n'001';2013-05-11;2015-07-03;100\\n'11111';2012-11-15;2999-21-31;1001\\n'11111';2007-08-01;2012-11-14;2002\\n'11111';2004-05-15;2007-07-31;3003\\n'11111';2002-03-16;2004-05-14;4004\\n'33';2020-03-12;2999-12-312002\\n'33'2019-01-31;2020-03-11;3003\\n'33';2019-01-31;2019-01-31;4004\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_df = sc.parallelize([('001','2018-08-11',100),\\\n",
    "                          ('001','2015-07-04',100),\\\n",
    "                          ('001','2013-05-11',100),\\\n",
    "                          ('11111','2012-11-15',1001),\\\n",
    "                          ('11111','2007-08-01',2002),\\\n",
    "                          ('11111','2004-05-15',3003),\\\n",
    "                          ('11111','2002-03-16',4004),\\\n",
    "                          ('33','2020-03-12',2002),\\\n",
    "                          ('33','2019-01-31',3003),\\\n",
    "                          ('33','2019-01-31',4004),])\\\n",
    "                .toDF(['inn','from_dt','cur_val'])\n",
    "\n",
    "\n",
    "'''--надо получить\n",
    "inn;from_dt;to_dt;cur_val\n",
    "'001';2018-08-11;2999-12-31;100\n",
    "'001';2015-07-04;2018-08-10;100\n",
    "'001';2013-05-11;2015-07-03;100\n",
    "'11111';2012-11-15;2999-21-31;1001\n",
    "'11111';2007-08-01;2012-11-14;2002\n",
    "'11111';2004-05-15;2007-07-31;3003\n",
    "'11111';2002-03-16;2004-05-14;4004\n",
    "'33';2020-03-12;2999-12-312002\n",
    "'33'2019-01-31;2020-03-11;3003\n",
    "'33';2019-01-31;2019-01-31;4004'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+-------+\n",
      "|  inn|   from_dt|     to_dt|cur_val|\n",
      "+-----+----------+----------+-------+\n",
      "|  001|2013-05-11|2015-07-03|    100|\n",
      "|  001|2015-07-04|2018-08-10|    100|\n",
      "|  001|2018-08-11|2999-12-31|    100|\n",
      "|11111|2002-03-16|2004-05-14|   4004|\n",
      "|11111|2004-05-15|2007-07-31|   3003|\n",
      "|11111|2007-08-01|2012-11-14|   2002|\n",
      "|11111|2012-11-15|2999-12-31|   1001|\n",
      "|   33|2019-01-31|2019-01-31|   3003|\n",
      "|   33|2019-01-31|2020-03-11|   4004|\n",
      "|   33|2020-03-12|2999-12-31|   2002|\n",
      "+-----+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# добавил order by по cur_val, и доработал даты (-1)\n",
    "wind_df.withColumn('to_dt', F.lead(wind_df['from_dt']).over(Window.partitionBy(wind_df.inn).orderBy('from_dt', 'cur_val')))\\\n",
    "       .withColumn('to_dt', F.when(col('from_dt') == col('to_dt'), col('to_dt')).otherwise(F.date_add(F.to_date('to_dt'), -1)))\\\n",
    "       .withColumn('to_dt', F.when(col('to_dt').isNull(), '2999-12-31').otherwise(col('to_dt')))\\\n",
    "       .select('inn', 'from_dt', 'to_dt', 'cur_val').orderBy('inn', 'to_dt').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF\n",
    "- Импортируем функционал udf в спарке\n",
    "- Создаем саму функцию с нужным нам функцилоналом (в питоне)\n",
    "- Регистрируем питон-функцию как UDF + возвращаемый тип значения, чтобы спарк мог ее использовать\n",
    "- Применяем UDF в спарк-коде "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_valid_inn(inn):\n",
    "    if len(inn) < 4:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "udf_is_valid_inn=udf(is_valid_inn, BooleanType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+--------------+\n",
      "|  inn|   from_dt|cur_val|is_correct_crc|\n",
      "+-----+----------+-------+--------------+\n",
      "|  001|2018-08-11|    100|         false|\n",
      "|  001|2015-07-04|    100|         false|\n",
      "|  001|2013-05-11|    100|         false|\n",
      "|11111|2012-11-15|   1001|          true|\n",
      "|11111|2007-08-01|   2002|          true|\n",
      "|11111|2004-05-15|   3003|          true|\n",
      "|11111|2002-03-16|   4004|          true|\n",
      "|   33|2020-03-12|   2002|         false|\n",
      "|   33|2019-01-31|   3003|         false|\n",
      "|   33|2019-01-31|   4004|         false|\n",
      "+-----+----------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wind_df.withColumn('is_correct_crc', udf_is_valid_inn(col('inn'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_list = ['33', '001']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### обычный фильтр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+\n",
      "|inn|   from_dt|cur_val|\n",
      "+---+----------+-------+\n",
      "|001|2018-08-11|    100|\n",
      "|001|2015-07-04|    100|\n",
      "|001|2013-05-11|    100|\n",
      "| 33|2020-03-12|   2002|\n",
      "| 33|2019-01-31|   3003|\n",
      "| 33|2019-01-31|   4004|\n",
      "+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_df = wind_df.where(wind_df.inn.isin(filter_list))\n",
    "filter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter inn#707 IN (33,001)\n",
      "+- Scan ExistingRDD[inn#707,from_dt#708,cur_val#709L]\n"
     ]
    }
   ],
   "source": [
    "filter_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### broadcast фильтр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fbl = sc.broadcast(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------+\n",
      "|inn|   from_dt|cur_val|\n",
      "+---+----------+-------+\n",
      "|001|2018-08-11|    100|\n",
      "|001|2015-07-04|    100|\n",
      "|001|2013-05-11|    100|\n",
      "| 33|2020-03-12|   2002|\n",
      "| 33|2019-01-31|   3003|\n",
      "| 33|2019-01-31|   4004|\n",
      "+---+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_droadcast_df = wind_df.where(wind_df.inn.isin(fbl.value))\n",
    "filter_droadcast_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*Filter inn#707 IN (33,001)\n",
      "+- Scan ExistingRDD[inn#707,from_dt#708,cur_val#709L]\n"
     ]
    }
   ],
   "source": [
    "filter_droadcast_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# обязательно гасим контекст в конце , иначе он будет висеть и забирать ресурсы все время.\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вывод директорий или файлов в порядке возрастания даты. \n",
    "#Удобно, если хочется почистить старые таблицы.\n",
    "\n",
    "!hdfs dfs -ls /data/adwh/prod_pim_dev_sbx/ | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вывод файлов созданных до/после определенной даты.\n",
    "\n",
    "!hdfs dfs -ls -t /data/adwh/prod_pim_dev_sbx/ | awk -v dateA=\"2021-11-01\" '{if ($6 < dateA) {print $8}}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалить директорию или табличку из паркетов\n",
    "\n",
    "!hdfs dfs -rmr -skipTrash /data/adwh/prod_pim_dev_sbx/*\n",
    "!hdfs dfs -rmr -skipTrash /data/adwh/prod_pim_dev_sbx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
